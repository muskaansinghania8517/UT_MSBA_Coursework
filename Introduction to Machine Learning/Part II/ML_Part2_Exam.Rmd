---
title: "Machine Learning Part 2 Exam"
author: "Sai Bhargav, Apurva Audi, Vivian Wang, Muskaan Singhania"
date: '2022-08-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 7, out.width = "57%", fig.height = 7, out.height = "57%")
```
**GitHub Link to RMD File**: https://github.com/muskaansinghania85/ML_Projects/blob/main/ML_Part2_Exam.Rmd

# Probability Practice

## Part a
P(Y) = 0.65 and P(N) = 0.35 (Y = Yes, N = No)

P(R) = 0.3 and P(T) = 0.7 (R = Random Clicker, T = Truthful Clicker)

P(Y|R) = 0.5 = P(N|R)

P(Y) = P(R) * P(Y|R) + P(T) * P(Y|T)

=> P(Y|T) = (P(Y) - P(R) * P(Y|R))/P(T)

Substituting the values we get:

P(Y|T) = (0.65 - 0.3 * 0.5)/0.7

**P(Y|T) = 0.7143** or **71.43%**

## Part b

P(P|D) = 0.993, P(N|no D) = 0.9999 (P = positive test, N = negative test, D = have disease, no D = have no disease)

P(D) = 0.000025 => P(no D) = 0.999975

P(D|P) = P(D,P)/P(P) = **P(P|D) * P(D)/P(P)**

In the highlighted expression we know P(P|D), P(D). For P(P):

P(P) = P(P|D) * P(D) + P(P|no D) * P(no D)

=> P(P) = P(P|D) * P(D) + P(P,no D)

=> P(P) = P(P|D) * P(D) + P(no D) - P(N,no D)

=> **P(P) = P(P|D) * P(D) + P(no D) - P(no D) * P(N|no D)**

In the highlighted equation we know all the terms on the right

P(P) = 0.993 * 0.000025 + 0.999975 - 0.999975 * 0.9999 = 0.0001248225

Now:

P(D|P) = 0.993 * 0.000025/0.0001248225

**P(D|P) = 0.1989** or **19.89%**

# Wrangling the Billboard Top 100

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
data = read.csv('billboard.csv')
data = data[,c('performer','song','year','week','week_position')]
head(data)
```

## Part a
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
library(dplyr)
top_100_count = data %>%
  group_by(performer,song) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

top_100_count[1:10,]
```

## Part b
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
library(ggplot2)
unique(data[,c('performer','song','year')]) %>%
  filter(year != '2021' & year != '1958') %>%
  group_by(year) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = year, y = count)) + geom_line() + labs(x = 'Year', y = 'Count of unique songs in Billboard 100')
```
From the graph we see that the number of unique songs that appeared in the Billboard Top 100 each year initially increased from 1958 to about 1966. Then it decreased dramatically from that year and does not seem to bounce back up until after 2001. The music diversity is constantly changing and perhaps what we see in the graph represents some kind of cyclical pattern of the music industry. Maybe it tends to have a period of "prosperity" followed by some kind of "depression".

## Part c
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
library(forcats)
top_100_count %>%
  filter(count >= 10) %>%
  group_by(performer) %>%
  summarize(song_count = n()) %>%
  arrange(desc(song_count)) %>%
  filter(song_count >= 30) %>%
  ggplot(aes(fct_reorder(performer, song_count),song_count)) + geom_bar(stat = "identity") + coord_flip() + labs(x = "Performer", y = "No. of songs with ten-week hit", title = "Performers with atleast 30 ten-week hit songs")
```
From the graph we see that there are 19 artists who have had at least 30 songs that were "ten-week hits." When you get closer to the top ranking, the range of the number of songs gets much wider. Everyone except the top 4 artists all have pretty similar number of ten-week hits songs. When it gets to Tim McGraw, who is fourth place on the graph, the difference starts to increase. Most noticeably, Elton John, the artist with the highest number of ten-week hits, out-competes other artists on the ranking by a lot. The range is about 22, and Elton John has about 8 more ten-week hits than Madonna, the second place on the list.

# Visual story telling part 1: green buildings

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
rm(list=ls())
gb_data = read.csv('greenbuildings.csv')
head(gb_data)

gb_data %>%
  ggplot(aes(x = leasing_rate)) + geom_histogram() + labs(x = "Occupancy", y = "No. of buildings")
```

The greenbuildings.csv data has been read in. As seen in the histogram for Occupancy we have some buildings which have very low occupancy (< 10%). So as done by the developer's on-staff, the buildings with occupancy less than 10% will be removed.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
gb_data_clean = gb_data %>%
  filter(leasing_rate >= 10)

cat('The no.of buildings with Occupancy less than 10% is:',nrow(gb_data) - nrow(gb_data_clean),'\n')

low_occupancy_green = gb_data %>%
  filter(leasing_rate < 10 & green_rating == 1) %>%
  nrow()
cat('The no.of buildings with Occupancy less than 10% and have green rating:',low_occupancy_green)

```

215 buildings have been removed from the dataset due very low. Also, only one of them have green rating.   As seen in the  and as done by the developer's on-staff, the buildings with occupancy less than 10% have been removed.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
gb_data_clean["green_rating_factor"] = as.factor(gb_data_clean[,"green_rating"])
ggplot(gb_data_clean, aes(x=green_rating_factor, y=Rent)) + 
    geom_boxplot()
```
We can see from the boxplots that the buildings with green rating do have a slightly higher rent compared to those that do not have green rating.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
med_gb = gb_data_clean %>%
  group_by(green_rating) %>%
  summarize(median_rent = median(Rent))
med_gb
```
The median values for the green buildings and non-green buildings also seem to be correct. But we need to confirm if the rent is mostly affected by the green rating and no other factors. 

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
gb_cor_data = gb_data[,c(5,3,4,6,7,8,9,10,11,14,15,16,17,18,19,20,21,22,23)]
ggcorrplot::ggcorrplot(cor(gb_cor_data))
```

From the correlation plot, we can see that *Rent* is highly correlated with *cluster_rent*. This means that the rent of a building depends on the area in which the building is located. So we need to see if, in each cluster the green building has the highest or atleast top 3 in terms of rent.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
#library(tidyr)
gb_max_rent = gb_data_clean %>%
  #arrange(cluster,desc(Rent)) #%>%
  group_by(cluster) %>%
  top_n(n = 3, wt = Rent) %>%
  group_by(cluster) %>%
  summarize(green_present = sum(green_rating))
  
gb_max_rent[1:10,]
```
We created a table of cluster number and a column called *green_present* which has two inputs i.e., **0** and **1**. 1 implies that the green building present in the corresponding cluster is in the top 3 in terms of the rent. O implies that the green building is not in the top 3.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
gb_max_rent %>%
  group_by(green_present) %>%
  summarize(count = n()) %>%
  mutate(green_present = if_else(green_present == 1,"Green building in top 3","Green building not in top 3")) %>%
  ggplot(aes(green_present,count)) + geom_bar(stat = "identity") +ylab("No. of clusters")
  
```

In maximum number of clusters green building is in the top 3 buildings in terms of rent. But there is a considerable number of clusters in which the green building is not even in the top 3. Let us see in how many clusters we have the green building actually at the top.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
gb_data_clean %>%
  #arrange(cluster,desc(Rent)) #%>%
  group_by(cluster) %>%
  top_n(n = 1, wt = Rent) %>%
  group_by(cluster) %>%
  summarize(green_top = sum(green_rating)) %>%
  group_by(green_top) %>%
  summarize(count = n()) %>%
  mutate(green_top = if_else(green_top == 1,"Green building","Non - Green building")) %>%
  ggplot(aes(green_top,count)) + geom_bar(stat = "identity") +ylab("No. of clusters")
```

As we can see, more clusters have a non-green building with the highest rent. This confirms that having a green rating does not necessarily lead to higher rents. Also, we can see that *age*, *class-a*, *class-b* and *renovated* columns have a possibility of confounding relationships with *Rent* and *green_rating*.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
gb_data_plots = gb_data[,c(5,8,9,10,11,14)]
gb_data_plots[,"green_rating"] = as.factor(gb_data_plots[,"green_rating"])
gb_data_plots[,"class_a"] = as.factor(gb_data_plots[,"class_a"])
gb_data_plots[,"class_b"] = as.factor(gb_data_plots[,"class_b"])
gb_data_plots[,"renovated"] = as.factor(gb_data_plots[,"renovated"])

ggplot(gb_data_plots,aes(age,Rent)) + geom_point()
ggplot(gb_data_plots,aes(green_rating,age)) + geom_boxplot()

ggplot(gb_data_plots,aes(renovated,Rent)) + geom_boxplot()
table(gb_data_plots[,c(3,6)])

ggplot(gb_data_plots,aes(class_a,Rent)) + geom_boxplot()
table(gb_data_plots[,c(4,6)])

ggplot(gb_data_plots,aes(class_b,Rent)) + geom_boxplot()
table(gb_data_plots[,c(5,6)])
```

We can see from the graphs that *green_rating* and *Rent* have similar correlations with *age*, *class-a*, *class-b* and *renovated*. 

*  As *age* increases *Rent* goes down. In the box plot between *green_rating* and *age*, *green_rating* **0** has higher median *age*. 

*  From the confusion matrix between *renovated* and *green_rating* we see that the probability of having a *green_rating* 1 goes down from **0.08** to **0.05** given the condition that *renovated* is **1**. This implies that if a building has undergone substantial renovations in its lifetime then it is far from having a green rating. That is again connected to the age of the building. Older buildings are usually renovated

* *Class-a* buildings have higher rents. The condition that the given building is *class-a* rated increases the probability of it being a green-rated building from **0.08** to **0.173**

* *Class-b* buildings have lower rents. The condition that the given building is *class-b* rated decreases the probability of it being a green-rated building from **0.08** to **0.03**.

Hence, we can conclude that there are confounding variables which might give the impression that green-rating leads to higher rents.


# Visual story telling part 2: Capital Metro data

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
rm(list=ls())
metro_data = read.csv('capmetro_UT.csv')
head(metro_data)
metro_data['timestamp'] <- as.POSIXct(metro_data[,'timestamp'], format = "%Y-%m-%d %H:%M:%S")
metro_data['Time'] = format(metro_data[,'timestamp'], format = "%H:%M:%S")
metro_data['Date'] = format(metro_data[,'timestamp'], format = "%Y-%m-%d")
```

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
library(dplyr)
library(ggplot2)
metro_data %>%
  group_by(Time,month) %>%
  summarize(mean_temp = mean(temperature)) %>%
  ggplot(aes(Time,mean_temp, color = month)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))
```
The above scatter plot is between time of the day and mean temperature at that time of the day in a particular month. We can clearly see that in all the three months the temperature starts at lowest point at 6 in the morning, rises to a peak in the afternoon and cools down throughout the evening and night. Also, we can see that Sep is hotter than October and October is hotter than November. This represents the arrival of winter season in November.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}

metro_data %>%
  group_by(hour_of_day,Date,month) %>%
  summarize(boarding = mean(boarding)) %>%
  group_by(hour_of_day,month) %>%
  summarize(mean_boarding = mean(boarding)) %>%
  ggplot(aes(hour_of_day,mean_boarding)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(month), dir = "v")

metro_data %>%
  group_by(hour_of_day,Date,month) %>%
  summarize(alighting = mean(alighting)) %>%
  group_by(hour_of_day,month) %>%
  summarize(mean_alighting = mean(alighting)) %>%
  ggplot(aes(hour_of_day,mean_alighting)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(month), dir = "v")
```

* Most of the boarding of the Capital Metro buses at the UT campus is happening in the evening hours when classes are over for most of the students and they are leaving from the campus

* Most of the alighting of the Capital Metro buses at the UT campus is happening in the morning hours when classes are going to start for most of the students and they are coming to the campus

We have the data from September to November. In all the three months we see similar trend for alighting and boarding. But, these trends might not be repeated in December because of the winter holidays which will lead to lower student population around the campus. But we do have weekends data which can represent December month.
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
metro_data %>%
  group_by(hour_of_day,weekend) %>%
  summarize(mean_boarding = mean(boarding)) %>%
  ggplot(aes(hour_of_day,mean_boarding)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(weekend), dir = "v")

metro_data %>%
  group_by(hour_of_day,weekend) %>%
  summarize(mean_alighting = mean(alighting)) %>%
  ggplot(aes(hour_of_day,mean_alighting)) + geom_point() + theme(axis.text.x = element_text(size = 7,angle = 90))+facet_wrap(~fct_rev(weekend), dir = "v")
```
We can see that during the weekends there is not much activity as there are no classes at UT on weekends.


# Portfolio modeling
In this exercise I am going to take three different portfolios. The first portfolio will have 5 ETFs which are considered to be very safe. In the second portfolio, I am going to consider 5 ETFs which are considered to be highly risky. And in the third I am going to take 3 safe and 2 risky ETFs. In all the portfolios, we are starting with an initial capital of $100,000 and checking the final return at the end of 20 days. 

## Portfolio 1
The ETFs considered are *SPDR Bloomberg 1-3 Month T-Bill (BIL)*, *iShares Short Treasury Bond (SHV)*, *Invesco Ultra Short Duration (GSY)*, *Goldman Sachs Access Treasury 0-1 Year (GBIL)* and *SPDR SSGA Ultra Short Term Bond (ULST)*. These ETFs are considered to be very safe and not much variation is expected in terms of returns. 

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
rm(list=ls())
library(mosaic)
library(quantmod)
library(foreach)

safestocks = c("BIL", "SHV", "GSY", "GBIL", "ULST")
prices_safe = getSymbols(safestocks, from = "2017-01-01")

for(ticker in safestocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

all_returns_safe = cbind(	ClCl(BILa),
								ClCl(SHVa),
								ClCl(GSYa),
								ClCl(GBILa),
								ClCl(ULSTa))
all_returns_safe = as.matrix(na.omit(all_returns_safe))

head(all_returns_safe)
```

As we can see, the change in the closing prices of these ETFs is very low. Let's use Bootstrap resampling and see the variation in the next 20 days.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_safe, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

hist(sim1[,n_days], 25, xlab = "Value after 20 days")
cat("The mean value we have at the end of 20 days is ",mean(sim1[,n_days]),"\n")

hist(sim1[,n_days]- initial_wealth, breaks=30, xlab = "Net Return after 20 days")
cat("The mean total variation is ", mean(sim1[,n_days] - initial_wealth), "\n")

# 5% value at risk:
cat("VaR is ",quantile(sim1[,n_days]- initial_wealth, prob=0.05)*-1)
```

As we were expecting, the variation in returns for this portfolio is very low and hence we get a low VaR. Also, the histogram for the returns has a low standard deviation which also corroborates to the fact that the ETFs in this portfolio are safe.

## Portfolio 2
The ETFs considered are *ProShares UltraPro QQQ (TQQQ)*, *ProShares Ultra QQQ (QLD)*, *Direxion Daily S&P 500 Bull 3x Shares (SPXL)*, *Direxion Daily S&P 500 Bull 2x Shares (SPUU)* and *Direxion Daily 20+ Year Treasury Bull 3x Shares (TMF)*. These ETFs are considered to be highly volatile.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
riskystocks = c("TQQQ", "QLD", "SPXL", "SPUU", "TMF")
prices_risky = getSymbols(riskystocks, from = "2017-01-01")

for(ticker in riskystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

all_returns_risky = cbind(	ClCl(TQQQa),
								ClCl(QLDa),
								ClCl(SPXLa),
								ClCl(SPUUa),
								ClCl(TMFa))
all_returns_risky = as.matrix(na.omit(all_returns_risky))

head(all_returns_risky)
```

We can see that the daily variation of these ETFs are high compared to the Portfolio 1 ETFs that we have seen before.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_risky, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

hist(sim2[,n_days], 25, xlab = "Value after 20 days")
cat("The mean value we have at the end of 20 days is ",mean(sim2[,n_days]),"\n")

hist(sim2[,n_days]- initial_wealth, breaks=30, xlab = "Net Return after 20 days")
cat("The mean total variation is ", mean(sim2[,n_days] - initial_wealth), "\n")

# 5% value at risk:
cat("VaR is ",quantile(sim2[,n_days]- initial_wealth, prob=0.05)*-1)
```

The variation in returns for this portfolio is high and hence we get a high VaR. Also, the histogram for the returns has a high standard deviation which also corroborates to the fact that the ETFs in this portfolio are volatile.

## Portfolio 3
The ETFs considered are *ProShares UltraPro QQQ (TQQQ)*, *ProShares Ultra QQQ (QLD)*, *SPDR Bloomberg 1-3 Month T-Bill (BIL)*, *iShares Short Treasury Bond (SHV)*, *Invesco Ultra Short Duration (GSY)*. We have 2 volatile and 3 safe ETFs in this portfolio. 

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
all_returns_mixed = cbind(	ClCl(TQQQa),
								ClCl(QLDa),
								ClCl(BILa),
								ClCl(SHVa),
								ClCl(GSYa))
all_returns_mixed = as.matrix(na.omit(all_returns_mixed))

head(all_returns_mixed)
```

We see that the variation in the closing prices for the two risky portfolios are in another scale compared to the three safe portfolios.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
initial_wealth = 100000
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_mixed, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

hist(sim3[,n_days], 25, xlab = "Value after 20 days")
cat("The mean value we have at the end of 20 days is ",mean(sim3[,n_days]),"\n")

hist(sim3[,n_days]- initial_wealth, breaks=30, xlab = "Net Return after 20 days")
cat("The mean total variation is ", mean(sim3[,n_days] - initial_wealth), "\n")

# 5% value at risk:
cat("VaR is ",quantile(sim3[,n_days]- initial_wealth, prob=0.05)*-1)
```

For this mixed portfolio, we can see that the VaR is lower compared to the *Portfolio 2* which had all the risky ETFs. We can also see that the histogram for the final return has more spread compared to the one in *Portfolio 1* but lower spread compared to the one in *Portfolio 2*.

# Clustering and PCA

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
rm(list=ls())
library(ggplot2)
library(foreach)
library(mosaic)
wine_data = read.csv('wine.csv')
head(wine_data)
cor_data = wine_data
cor_data[,13] = ifelse(cor_data[,13] == 'red',1,0)
ggcorrplot::ggcorrplot(cor(cor_data), hc.order = TRUE)
```
The *color* column has been modified to binary representation. If the wine is red it will be **1** else **0**. Now, since all the columns are numerical a correlation plot has been created to see the correlation between the columns of interest (*color* and *quality*). We notice for *color* column has high positive correlation with *volatile.acidity* and *fixed.acidity*.  
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
X = wine_data[,-c(12,13)]
X_scaled = scale(X, center=TRUE, scale=TRUE)


mu = attr(X_scaled,"scaled:center")
sigma = attr(X_scaled,"scaled:scale")

cluster_color = kmeans(X_scaled, 2, nstart=25)
qplot(fixed.acidity, volatile.acidity, data=wine_data, color=factor(cluster_color$cluster))
qplot(fixed.acidity, volatile.acidity, data=wine_data, color= color)
```

The first scatter plot is between *fixed.acidity* and *volatile.acidity* columns from the Wine data. These two columns are selected because they both have a positive correlation with color column. Which means for higher values, the wine is going to be red wine. The colors are based on the clusters each data point is assigned to using K-means clustering. The second plot is between the same attributes but the colors are based on whether the datapoint corresponds to a red wine or a white wine. When we compare the two plots we can see that with a few exceptions, the datapoints have been clustered correctly.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
cluster_quality = kmeans(X_scaled, 7, nstart=25)
qplot(pH, alcohol, data=wine_data, color=factor(cluster_quality$cluster))
qplot(pH, alcohol, data=wine_data, color= factor(quality))
```

The plots are between *pH* and *alcohol* but in the first plot, the color coding is based on the clusters each of the data points have been assigned to. The color coding the second plot is based on the quality rating of the wine. From the plots we can see that the k-means clustering did not do a good job in clustering the wines to correct qualities. We will use PCA (Principal Components Analysis) to see what components come up.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
pca = prcomp(X_scaled, scale=TRUE, rank=1)
summary(pca)
print('Coefficients of x variables to get Principal Component 1')
pca$rotation[,1]

qplot(fixed.acidity, volatile.acidity, data=wine_data, color= color)
qplot(fixed.acidity, volatile.acidity, data=wine_data, color=pca$x[,1])+scale_color_gradient(low = 'blue', high='red')
```
We see that from the coefficents of x variables for PC1 it seems like if it is positive it is mostly white wine else it is a red wine. This can be interpreted by comparing the coefficient values and the correlation between color and other features. For example, the coefficient for *total.sulfur.dioxide* is positive and the highest in magnitude. If we see in the correlation matrix plot for *color* and *total.sulfur.dioxide* has a very negative correlation. The direction is opposite as PC1 is measuring the whiteness of the wine whereas the color column used for the correlation matrix is taking red wine as 1.
A scatter plot between fixed.acidity and volatile.acidity has been created twice. Once with the color of the points determined by the type of wine column (red or white). In the second graph, the color of the points is determined by the value of PC1. The type of wine is clearly visible in the plot with the PC1 based gradient coloring.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
qplot(pH, alcohol, data=wine_data, color= factor(quality))
qplot(pH, alcohol, data=wine_data, color=pca$x[,1])+scale_color_gradientn(colors = rainbow(7))
```
We again plotted two scatter plots but this time it is between *pH* and *alcohol*. The first graph is color coded by the quality rating of the wine. The second graph is gradient color coded based on PC1 value. Compared to K-means clustering the quality of wines is more accurately divided by PCA with one Principal Component.

# Market Segmentation 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Importing the required libraries
rm(list=ls())
library(tidyverse) 
library(cluster)    
library(factoextra)
library(NbClust)
library(corrplot)
library(gridExtra)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Reading the required data
sm_df = read.csv('social_marketing.csv')
head(sm_df)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sm_df = sm_df[, !(colnames(sm_df) %in% c("chatter","uncategorized"))]
```
We have dropped the irrelevant columns i.e Chatter and uncategorized since they wouldn't give out any information to make the segment
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Removed the irrelevant columns (Uncategorized, Chatter) so as to not skew the analysis
sm_df = sm_df[sm_df$spam == 0 & sm_df$adult == 0,] 
sm_df = sm_df[, !(colnames(sm_df) %in% c("adult","spam"))]
```
We have also dropped users who have been identified as spam or adult even once since the question mentions that even though they have filtered out, there could have been some slip through and we don't want these bots to skew our analysis
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Dropping rows where the user's tweet might have been identified as Spam/ adult so as to not skew our analysis

#install.packages("remotes")
#remotes::install_github("laresbernardo/lares")
# library(lares)
# 
# corr_cross(sm_df[2:33], 
#   max_pvalue = 0.05, 
#   top = 10 )

cormat <- cor(sm_df[c(2:33)])
corrplot(cormat,method = 'color',order = 'alphabet', type = 'upper', diag = FALSE)
```
We can see **correlated categories** above. For example, it makes sense that users who are into health nutrition are also into personal fitness. We're sure there are more correlated categories that would become more evident once we perform cluster analysis
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Normalizing the data to perform clustering
sm_scaled <- scale(sm_df[,2:33], center=TRUE, scale=TRUE)

mu = attr(sm_scaled,"scaled:center")
sigma = attr(sm_scaled,"scaled:scale")
```
We have normalized the data to perform clustering since it's distance based algorithm
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
#Finding the optimal number of clusters

# Elbow method
wss = fviz_nbclust(sm_scaled, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

# Silhouette method
sil = fviz_nbclust(sm_scaled, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

grid.arrange(wss,sil,ncol = 2) 
```

It isn't very clear from the elbow plot as to what number of clusters is the most optimal. Silhoutte method recommends two clusters, but for our use case that's too small a number

# Forming the clusters 
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
# 4 clusters
set.seed(42)
clusters_4 = kmeans(sm_scaled, 4, nstart=25)

# Visualizing the clusters on a dimensionally reduced plot
cluster_4_plot = fviz_cluster(clusters_4, data = sm_scaled, 
             ellipse.type = "euclid", # Concentration ellipse
             ggtheme = theme_classic(),geom = c("point")
             )

# 5 clusters
set.seed(42)
clusters_5 = kmeans(sm_scaled, 5, nstart=25)

# Visualizing the clusters on a dimensionally reduced plot
cluster_5_plot = fviz_cluster(clusters_5, data = sm_scaled, 
             ellipse.type = "euclid", # Concentration ellipse
             ggtheme = theme_classic(),geom = c("point")
             )


# 6 clusters
set.seed(42)
clusters_6 = kmeans(sm_scaled, 6, nstart=25)
# Visualizing the clusters on a dimensionally reduced plot
cluster_6_plot = fviz_cluster(clusters_6, data = sm_scaled, 
             ellipse.type = "euclid", # Concentration ellipse
             ggtheme = theme_classic(),geom = c("point")
             )
grid.arrange(cluster_4_plot,cluster_5_plot,cluster_6_plot, ncol = 3)
```
After visually examining the plot above, we notice that the **green squares in the 6 clusters plot don't seem to make a significant cluster**. Even the though the clusters are more evident in the 4 clusters plot, I think it's better to **move forward with 5 clusters** as any insignificant cluster after profiling can be discarded
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
# Profiling Customers
result_cluster = aggregate(sm_df, by=list(cluster=clusters_5$cluster), mean)
result_cluster = as.data.frame(result_cluster)
result_cluster_t <- t(result_cluster)

colnames(result_cluster_t) <- rownames(result_cluster)
rownames(result_cluster_t) <- colnames(result_cluster)

result_cluster_t = result_cluster_t[-2,]
result_cluster_t = result_cluster_t[-1,]
```
We assigning features to clusters based on the frequency of posts for a specific category as compared to mean of other clusters 
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
# Feature Analysis of Clusters
k = colnames(result_cluster_t)[apply(result_cluster_t,1,which.max)]
clus_features = cbind(rownames(result_cluster_t),k)
clus_features[order(k),]
```
One of clusters got wiped out since it didn't have any significant result  

From the output above, we came up with the following the customer profiles   

**Health Conscious Segment**: They seem to be enthusiastic about Health, Nutrition and Fitness  
**Family Focused Segment**: Their interests are usually surrounding schooling, parenting, religion, home and garden  
**Millenials/ Gen-Zers**: They are into things like music, shopping, online shopping, photo sharing, beauty, fashion, college uni, playing sports, start ups, online gaming etc  
**Knowledgeable Segment**: They like tweeting about business, news, politics, automative, travel & tv/film

# The Reuters Corpus

Data was received in a format where a train folder contained folders for 50 authors and each author folder contained 50 documents for each author. It also contained a test folder with a similar structure with the same 50 authors and 50 other documents written by each author.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
rm(list =ls())
# install.packages("tm")
# install.packages("wordcloud")
# install.packages("magrittr")
# install.packages("slam")
# install.packages("proxy")
# install.packages("e1071")
# install.packages("dplyr")
# install.packages("caret")
 # install.packages("naivebayes")
library(tm)
library(tidyverse)
library(magrittr)
library(slam)
library(proxy)
library(e1071)
library(dplyr)
library(caret)
library(naivebayes)
library(wordcloud)
library(cluster)

readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)),
							id=fname, language='en') }
```
## Method  

First we read the author folder names and looped through the contents to get the document information and content. This was done for both train and test sets. With this data we made a corpus document, which is a text mining document. We then mapped five content transformers to make everything lowercase and remove numbers, punctuation, excess whitespace and stop words. Next, we created a document term matrix that is a data frame like structure that comprises rows for each document and columns for all terms that appear across documents. Once this was created, sparse terms were removed on the training and test document term matrices using 91% and 96% thresholds respectively (these thresholds are somewhat arbitrary and different values could be used). Finally, we removed terms that occur in the test data but not in the train data.

Training files : 
+ Clean up the file names in training set
+ Renaming the articles
+ Creating vector for documents

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
authors_train = Sys.glob('ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL

for(author in authors_train) {
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  author_name = substring(author, first=23)
  labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}

train = lapply(file_list_train, readerPlain)

mynames = file_list_train %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(train) = mynames

```
Testing files : 
+ Clean up the file names in testing set
+ Renaming the articles
+ Creating vector for documents

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
authors_test = Sys.glob('ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL

for(author in authors_test) {
  files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add_test)
  author_name_test = substring(author, first=23)
  labels_test = append(labels_test, rep(author_name_test, length(files_to_add_test)))
}

test = lapply(file_list_test, readerPlain)

mynames = file_list_test %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(test) = mynames

```
After creating a vector of all documents, we create a corpus for text
mining:

Creating a text mining corpus for train and transforming train corpus by:

-   Converting all text to lowercase
-   Removing all numbers to enable efficient text mining
-   Remove all punctuation like ',' '.'
-   Remove extra white-spaces
-   Removing common stop words as part of list "en"

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
doc_raw = Corpus(VectorSource(train))
doc=doc_raw

#Transforming train corpus
doc = tm_map(doc, content_transformer(tolower))
doc = tm_map(doc, content_transformer(removeNumbers))
doc = tm_map(doc, content_transformer(removePunctuation))
doc = tm_map(doc, content_transformer(stripWhitespace)) 
doc = tm_map(doc, content_transformer(removeWords), stopwords("en"))

```
Creating corpus for test documents

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
doc_raw_test = Corpus(VectorSource(test))
doc_test = doc_raw_test

#Transforming train corpus
doc_test = tm_map(doc_test, content_transformer(tolower))
doc_test = tm_map(doc_test, content_transformer(removeNumbers))
doc_test = tm_map(doc_test, content_transformer(removePunctuation))
doc_test = tm_map(doc_test, content_transformer(stripWhitespace)) 
doc_test = tm_map(doc_test, content_transformer(removeWords), stopwords("en"))

```

Document-Term Matrix for train and test corpus


```{r,echo=FALSE,warnings=FALSE,message=FALSE}
DTM_train = DocumentTermMatrix(doc)
DTM_test = DocumentTermMatrix(doc_test)
class(DTM_train)
```

Inspecting the entries of Document Term Matrix for train

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
inspect(DTM_train[1:10,1:20])
```
Frequently occurring words (count of atleast 1000) in Train corpus

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
word_freq_1000 = findFreqTerms(DTM_train, 1000)
word_freq_1000[1:10]
```

Finding words whose count correlates with a specific word
Interesting to observe that human character is depicted by authors as apolitical, rivaling etc with most words negatively portraying their character with each word of association more than 0.5

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
findAssocs(DTM_train, "character", .5) 
```

Building a word cloud to represent the words that have appeared atleast 50 times in the 15th author i.e. Jan Lapotka.
The words depit that his writings are around Finance and new practices in banking and trade.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
DTM.matrix = as.matrix(DTM_train)
wordcloud(colnames(DTM.matrix), DTM.matrix[15,], max.words = 50)
```


```{r,echo=FALSE,warnings=FALSE,message=FALSE}
print("Document Term Matrix for train")
DTM_train
print("Document Term Matrix for test")
DTM_test
```

Dropping terms that occur less frequently leading to a long tail of rare
terms

Removing words from train and test DT Matrix which have word count as 0
in more than 90% of documents in train and test.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
DTM_train <- removeSparseTerms(DTM_train, 0.9)
DTM_test <- removeSparseTerms(DTM_test, 0.9)
```

TF-IDF weights
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
tfidf_train = weightTfIdf(DTM_train)
tfidf_train
```

Compare documents with terms

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
inspect(tfidf_train[1,])
```

**Dimensionality Reduction**

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
X = as.matrix(tfidf_train)
summary(colSums(X))
red_cols = which(colSums(X) == 0)
X=X[,-red_cols]

pca_train = prcomp(X, rank=2, scale=TRUE)
plot(pca_train)
```

This plot clearly indicates that the first two PCA trains explain maximum variability in the data and hence can be used for our analysis.

Description of Loadings in the PCA train 1 and 2

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
pca_train$rotation[order(abs(pca_train$rotation[,1]),decreasing=TRUE),1][1:25]
pca_train$rotation[order(abs(pca_train$rotation[,2]),decreasing=TRUE),2][1:25]
```

Each document into a single pair of numbers -- massive dimensionality reduction

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
#pca_train$x[,1:2]

plot(pca_train$x[,1:2], xlab="PCA 1 direction", ylab="PCA 2 direction", bty="n",
     type='n')
text(pca_train$x[,1:2], labels = 1:length(train), cex=0.7)

```

Distance Matrix using PCA scores

+ Forming hierarchical cluster of size 5 using Euclidean distance

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
dist_mat = dist(pca_train$x)
tree_train = hclust(dist_mat)
plot(tree_train)
clust5 = cutree(tree_train, k=5)
#inspect clusters
which(clust5 == 3)
```
Converting to dataframe
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
DTM_train <- as.data.frame(as.matrix(DTM_train))
DTM_test <- as.data.frame(as.matrix(DTM_test))
```
After removing sparseness
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
print("Dataframe of train")
dim(DTM_train)
print("Dataframe of test")
dim(DTM_test)
```
**Models** 

**Naive Bayes Classifier**

First we ran Naive-Bayes to predict authors. The data was changed to matrices to run the Naive-Bayes library and had predictive accuracy of approximately 33.28% . This uses the naivebayes library in R. 


```{r,echo=FALSE,warnings=FALSE,message=FALSE}
DTM_test_norm <- DTM_test[ ,(names(DTM_test) %in% names(DTM_train))]
#train = as.matrix(DTM_train_freq)
train = as.matrix(DTM_train)
test =  as.matrix(DTM_test_norm)
train_Y =  as.factor(labels_train)
test_Y = as.factor(labels_test)
train_df = cbind(as.data.frame(train), as.data.frame(train_Y))
classifier <-  naive_bayes(train, train_Y, laplace = 1)
pred <- predict(classifier, test, type = 'class')
pred_tab <- table("Predictions"= pred,  "Actual" = labels_test)
NB_r <- sum(diag(pred_tab))/sum(pred_tab)

NB_r
```

## Random Forest Classifier

Secondly we ran a Random Forest model with 1,250 trees and an m of 20. To run this in R, we had to change some terms in the train and test data sets so they did not conflict with special terms in R. The Random Forest model had predictive accuracy of approximately 54.12% . This uses the randomForest library in R.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
library(randomForest)

cols = intersect(names(DTM_test), names(DTM_train))
DTM_test = DTM_test %>% select(cols)
DTM_train = DTM_train %>% select(cols)

model_rf <- randomForest::randomForest(x = DTM_train, y= as.factor(labels_train), ntree=1250, mtry=20) # Non cv
pred_rf <- predict(model_rf, DTM_test)
head(pred_rf)

pred_tab_rf <- table("Predictions"= pred_rf,  "Actual" = labels_test)
rf_r <- sum(diag(pred_tab_rf))/sum(pred_tab_rf)

rf_r
```

# Association Rule Mining
```{r,echo=FALSE,warnings=FALSE,message=FALSE}
rm(list=ls())

# install.packages("arules")
# install.packages("arulesViz")

file = "groceries.txt"
library(tidyverse)
library(arules)
library(arulesViz)

df = read.table(file, sep = ',', header = FALSE, fill = TRUE)

library(reshape2)
#add a user column
add_user = tibble::rowid_to_column(df, "User")

#combine all items in one column
df2 = melt(add_user, id.vars = c("User"))
df2$variable = NULL
attach(df2)
#reorder column by each user's basket
df2 = df2[order(User),]
detach(df2)
#remove all empty cells
df2 = df2[!apply(df2 == "", 1, any),]


# Barplot of top 20 items

#get each item's frequency
frq = table(df2$value)
head(frq)
dffrq = as.data.frame(frq)

# sort
attach(dffrq)
#sort by descending frequency
dffrq = dffrq[order(-Freq),]
#plot top 20 items
barplot(dffrq$Freq[1:20], names=dffrq$Var1[1:20], las=2, cex.names=0.6)
detach(dffrq)


# Turn user into a factor
df2$User = factor(df2$User)
# Split data into a list of grocery items for each user
grocs = split(x=df2$value, f=df2$User)
# Remove duplicates
grocs = lapply(grocs, unique)
# Cast as "transactions"class
grocstrans = as(grocs, "transactions")
summary(grocstrans)

#run the 'apriori' algorithm
grocrules = apriori(grocstrans,
                     parameter=list(support=.005, confidence=.1, maxlen=5))

# 118 rules in total
inspect(grocrules)
# Choose a subset
inspect(subset(grocrules, subset=lift > 3.5))
inspect(subset(grocrules, subset=confidence > 0.3))
inspect(subset(grocrules, subset=lift > 2.5 & confidence > 0.3))
# Plot
plot(grocrules)
# can swap the axes and color scales
plot(grocrules, measure = c("support", "lift"), shading = "confidence")
# "two key" plot: coloring is by size (order) of item set
plot(grocrules, method='two-key plot')
# can now look at subsets driven by the plot
inspect(subset(grocrules, support > 0.035))
inspect(subset(grocrules, confidence > 0.7))
inspect(subset(grocrules, lift > 20))

# graph-based visualization
sub1 = subset(grocrules, subset=confidence > 0.02 & support > 0.005 & lift > 2)
summary(sub1)
plot(sub1, method='graph')

```
From the bar graph we see the **top 20 most purchased grocery items**. These are common goods that people buy on a regular basis like **milk, vegetables, soda, water, and fruit**. 

When looking at the subset of the **association rules where lift > 3.5**, we see that there are only **five pairs of items**, which makes sense because we are picking a high threshold for lift. These are pairs where there is a strong "relationship" between the LHS and the RHS. This means that buyers are much more likely to buy the RHS items if they buy the LHS items. So the five pairs filtered are **(Onions, root vegetables), (beef, root vegetables), (root vegetables, beef), (pip fruit, tropical fruit), (tropical fruit, pip fruit)**. Out of these 5, **two are duplicates**. So there are only **three pairs of items that have a lift greater than 3.5**. When looking at the subset where **confidence is greater than 0.3**, we see that there are **9 pairs of items**. It's interesting to note that for each of these pairs, the **confidence is greater than the support**. In other words, these **pairs are more like complementary goods**, as opposed to the example we looked at in class where coffee and tea are substitutes. When looking at a **combination of lift greater than 2.5 and confidence greater than 0.3, we see that for all of them support is low, confidence is high and lift is high**.

Futhermore, they are all pointing to the same item, other vegetables. When plotting the rules, we see that high lift rules usually have low support. The two-key plot illustrates that **order 3 rules tend to have high confidence**. This makes sense because these are probably **popular grocery items** that people purchase other things with. When looking at pairs of items where **support is greater than 0.035, we see only six pairs**. **4 of them have empty LHS's**. **For the remaining two**, confidence is higher than support and lift is at about 2, indicating these are **complementary goods**. And these two are actually **duplicate pairs**. However, it's interesting that even though their support is the same, they have slightly different confidence values. **From the plot of the subset where confidence > 0.02, support > 0.005, and lift > 2, we can see that the different kind of meats are clustered together, same thing with dairy products, and vegetables**.

